_free_text_description: "testing dual tower architecture PPI with AA representation for both inputs"

seed_everything: null

only_get_expected_working_dir: false

task_info:
  class_label_to_idx: 
    Inactive: 0
    Inconclusive: 1
    Active: 2
    Unspecified: 3
    Probe: 4

clearml:  
  active: true
  project_name: PPI_affinity/BMCA
  task_name: ${current_username:}  
  load_from_checkpoint_continues_within_session: true

session_group_name: ${clearml.project_name}@${clearml.task_name}

paths:    
  session_dir: ${cwd:}/run_${local_time:Israel}
  #will be used if no session manager is involved. Useful for quick debugging
  #if you modify it be aware that the code may "climb up" 1-2 directory levels up and create files/dirs there.
  debug_session_dir: _YOUR_WORK_PATH_/${current_username:}/debug_session/DEPLOYED_dummy/run_dummy

hydra:
  run:
    dir: ${paths.session_dir}
    # You can override this value when running run.py - for example: python run.py hydra.run.dir='/some/path/you/want'
    # this approach is especially useful when you run a daemonized process which has its stdout/err redirected to a log file within this dir. 
    # alternatively, you can use multi-run (looks useful for grid searches etc.)  - read: https://hydra.cc/docs/configure_hydra/workdir/#configuration-for-multirun
    # it uses hydra.sweep.dir and hydra.sweep.subdir (which uses ${hydra.job.num})

caching:
  dir: _YOUR_WORK_PATH_/${current_username:}/caching/dti_classifier/

trainer:
  # logger: true
  # checkpoint_callback: null
  # enable_checkpointing: true
  # callbacks: null
  # default_root_dir: null
  # gradient_clip_val: null
  # gradient_clip_algorithm: null
  # process_position: 0
  # num_nodes: 1
  # num_processes: 1
  # devices: null
  gpus: 1
  # auto_select_gpus: false
  # tpu_cores: null
  # ipus: null
  # log_gpu_memory: null
  # progress_bar_refresh_rate: null
  # enable_progress_bar: true
  # overfit_batches: 0.0
  # track_grad_norm: -1
  # check_val_every_n_epoch: 1
  # fast_dev_run: false
  # accumulate_grad_batches: null
  max_epochs: 1000 #99999
  # min_epochs: null
  # max_steps: -1
  # min_steps: null
  # max_time: null
  # limit_train_batches: 1.0
  # limit_val_batches: 1.0
  # limit_test_batches: 1.0
  # limit_predict_batches: 1.0
  # val_check_interval: 1.0
  # flush_logs_every_n_steps: null
  # log_every_n_steps: 50
  # accelerator: null
  # strategy: null
  # sync_batchnorm: false
  # precision: 32
  # enable_model_summary: true
  # weights_summary: top
  # weights_save_path: null
  # num_sanity_val_steps: 2
  # resume_from_checkpoint: null
  # profiler: null
  # benchmark: false
  # deterministic: false
  # reload_dataloaders_every_n_epochs: 0
  # reload_dataloaders_every_epoch: false
  auto_lr_find: false
  # replace_sampler_ddp: true
  # detect_anomaly: false
  # auto_scale_batch_size: false
  # prepare_data_per_node: null
  # plugins: null
  # amp_backend: native
  # amp_level: null
  # move_metrics_to_cpu: false
  # multiple_trainloader_mode: max_size_cycle
  # stochastic_weight_avg: false
  # terminate_on_nan: null

load_from_checkpoint: null 

model:  

  ####
  base_model: "bimodal_mca" 
  
  ligand_attention_size: 16
  receptor_attention_size: 16

  ligand_embedding_size: 35
  receptor_embedding_size: 35
  
  ####

  #temperature: 1.3

  #ligand
  ligand_embedding: "learned" 
  #ligand_embedding_size: 32
  
  ligand_filters: [256, 256, 256]    
  # ligand_kernel_sizes:
  #   - 
  #     - 3
  #     -  ${model.ligand_embedding_size}
  #   -
  #     - 5
  #     - ${model.ligand_embedding_size}
  #   - 
  #     - 11
  #     - ${model.ligand_embedding_size}

  ###ligand_attention_size: 64
  

  ligand_vocabulary_size: 28
  #ligand_vocabulary_size: 3000
  ligand_padding_length: 696
  
    
  #receptor
  receptor_embedding: "learned"
  #receptor_embedding_size: 16  
  
  receptor_filters: [256, 256, 256]      
  
  receptor_vocabulary_size: 28  
  receptor_padding_length: 2536
  
  dense_hidden_sizes: [256]
  activation_fn: "relu"
  final_activation: false
  loss_fn: "mse"
  dropout: 0.3
  batch_norm: true
  #batch_size: 32
  learning_rate: 1e-04
  # learning_rate: 0.001

  



data:
  
  lightning_data_module:       

    # ### kinase active-sites
    # ### AA sequence version 
    peptides_smi: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/epitopes.csv"
    proteins_smi: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/tcr.csv"
    train_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/train.csv"
    val_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/test.csv"
    test_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/test.csv"
    protein_representation_type: "AA" #'AA' or 'SMILES'
    peptide_representation_type: "AA" #'AA' or 'SMILES'

    ### SMILES peptide version
    # peptides_smi: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/epitopes.smi"
    # proteins_smi: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/tcr.csv"
    # train_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/train.csv"
    # val_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/test.csv"
    # test_dataset_path: "_YOUR_WORK_PATH_/${current_username:}/dev/data/TITAN/public/strict_split/fold0/test.csv"
    # protein_representation_type: "AA" #'AA' or 'SMILES'
    # peptide_representation_type: "SMILES" #'AA' or 'SMILES'

    pairs_table_ligand_column: ligand_name #ligand (peptide) column name in the pairs table (train/val/test)
    pairs_table_sequence_column: sequence_id #protein column name in the pairs table
    pairs_table_affinity_column: label #affinity column name in the pairs table

    ligand_vocabulary_size: ${model.ligand_vocabulary_size}
    target_vocabulary_size: ${model.receptor_vocabulary_size}
    receptor_vocabulary_size: ${model.receptor_vocabulary_size}
    
    ligand_padding_length: ${model.ligand_padding_length}
    target_padding_length: ${model.receptor_padding_length}
    receptor_padding_length: ${model.receptor_padding_length}

    # sample_pipeline_desc:
    #   - 
    #     _target_: fusedrug.data.molecule.ops.SmilesToRDKitMol


    
    # tokenizer related
    pytoda_SMILES_tokenizer_json: "_YOUR_WORK_PATH_/${current_username:}/dev/code/paccmann/paccmann_datasets/pytoda/smiles/metadata/tokenizer/vocab.json"
    pytoda_target_target_tokenizer_amino_acid_dict: "human-kinase-alignment"
    pytoda_wrapped_tokenizer: true

    # sample processing pipeline related

    active_site_alignment_info_smi: null #"/gpfs/haifa/projects/m/msieve/MedicalSieve/mol_bio_datasets/paccmann_related/active_sites_alignment_from_Tien_Huynh/joint_alignment_info.smi"
    train_augment_peptide_shuffle_atoms: false    
    train_augment_protein_flip: true

    protein_augment_full_sequence_noise: true #Whether or not to add noise to protein/peptide representation. TODO: split between proteins and peptides
    protein_augment_full_sequence_noise_p: 0.1 #Noise probability
    
    train_batch_size: 128
    eval_batch_size: 512
    num_workers: 4    
    train_shuffle: true
ckpt_path: _YOUR_WORK_PATH_/${current_username:}/dev/output/TITAN_fuse/yoels/bimca_affinity_predictor/sessions/fuse_based/models/pkbr/pretrain/val_rmse-v3.ckpt

