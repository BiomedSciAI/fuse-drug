hydra:
  run:
    dir: ${experiment.dir}
  job:
    chdir: False
experiment:
  project_name: PLM-DTI
  experiment_name: benchmark_data_lenient_split
  dir: ${paths.results}/${experiment.project_name}/${experiment.experiment_name}
  verbosity: 3
  seed: 0
  task: benchmark # "dti_dg" for TDCommons domain generalization benchmark,
            # one of EnzPredDataModule.dataset_list() ("halogenase",
            # "bkace", "gt", "esterase", "kinase", "phosphatase"),
            # "benchmark" for our large DTI benchmark data curation,
            # others - "davis", "bindingdb", "biosnap", "biosnap_prot" (unseen protein),
            # "biosnap_mol" (unseen drug), "dude" (not implemented)
  clearml: false
  only_load_checkpoint_weights: true # if "checkpoint" is provided, specify whether to only load weights (true) or entire training state (lr schedule etc'.) (false) 
paths:
  results: '${oc.env:DTI_RESULTS}'

benchmark_data:
  ### for debugging ###
  # to use a small data subset set limit_postfix: _limit_100000_rows
  # to also avoid pre loading features (use dummy instead) that takes a while set featurizer_debug_mode: True
  featurizer_debug_mode: False # if true, use dummy string in featurizer to speed up debugging
  limit_postfix: "" # "" for usual run, _limit_100000_rows for debug run with small files
  ############################################################################################################
  
  # parameters specific for our data/benchmark. used if task="benchmark"
  version: 14-04-2022_30_05-09-2022
  root_path: ${oc.env:DTI_BENCHMARK_DATA}/merged_${benchmark_data.version}
  pairs_tsv: ${benchmark_data.root_path}/${benchmark_data.version}/pairs/merged_bindingdb_chembl_pubchem_${benchmark_data.version}@native@single_protein_target@affinity_pairs_v0.1${benchmark_data.limit_postfix}.tsv
  ligands_tsv: ${benchmark_data.root_path}/${benchmark_data.version}/ligand/merged_bindingdb_chembl_pubchem_${benchmark_data.version}@native@single_protein_target@ligands${benchmark_data.limit_postfix}.tsv
  targets_tsv: ${benchmark_data.root_path}/${benchmark_data.version}/target/merged_bindingdb_chembl_pubchem_${benchmark_data.version}@native@single_protein_target@targets${benchmark_data.limit_postfix}.tsv
  # lenient split:
  splits_tsv: ${benchmark_data.root_path}/${benchmark_data.version}/splits/splits/${benchmark_data.split}/split_lenient_0.14_0.14_0.14_0.14_0.14_0.15_0.15${benchmark_data.limit_postfix}.tsv
  # cold_ligand split:
  #splits_tsv: ${benchmark_data.root_path}/splits/splits/${benchmark_data.split}/split_cold_ligand_0.7_0.15_0.15.tsv
  # cold_target split:
  #splits_tsv: ${benchmark_data.root_path}/splits/splits/${benchmark_data.split}/split_cold_target_0.7_0.15_0.15.tsv
  # temporal split:
  #splits_tsv: ${benchmark_data.root_path}/splits/splits/${benchmark_data.split}/split_temporal_0.7_0.15_0.15.tsv
  split: lenient # split if using task="benchmark". can be "lenient", "cond-target", "cold-ligand", "temporal".
  train_folds:
  - train1
  - train2
  - train3
  - train4
  - train5
  val_folds:
  - val # the '-' is needed so that it's used as a list
  test_folds:
  - test
  class_label_to_idx: 
    Active: 1
    Inactive: 0
  minibatches_per_epoch: 2500 # for debugging you can set a smaller value like 250
  validation_epochs: 50 # for debugging you can set a smaller value like 5
 
data:
  batch_size: 32
  contrastive_batch_size: 256
  shuffle: True
  num_workers: 16

model:
  # possible values: MorganFeaturizer, others (not tested): Mol2VecFeaturizer, GNN, MolEFeaturizer, MolRFeaturizer
  drug_featurizer: MorganFeaturizer
  # possible values: ProtBertFeaturizer, others (not tested): BeplerBergerFeaturizer, ESMFeaturizer, ProseFeaturizer, ProtT5XLUniref50Featurizer,
  #                  BindPredict21Featurizer, DSCRIPTFeaturizer
  target_featurizer: ProtBertFeaturizer
  # possible values: SimpleCoembedding, others (not tested): GoldmanCPI, SimpleCosine, AffinityCoembedInner, CosineBatchNorm, LSTMCosine, DeepCosine,
  #                  SimpleConcat, SeparateConcat, AffinityEmbedConcat, AffinityConcatLinear
  model_architecture: SimpleCoembedding
  latent_dimension: 1024
  latent_distance: "Cosine"

trainer:
  loss: 'focal' # 'bce' (binary cross-entropy), 'focal'
  epochs: 100 
  every_n_val: 2
  lr: 1e-4
  lr_t0: 10
  contrastive: false # Note: currently not implemented. If true, will be used with DUDE dataset
  contrastive_split: within
  clr: 1e-5
  clr_t0: 10
  device: 0 # 5

test:
  save_preds_for_benchmark_eval: false
# to run test only mode with an already trained model, provide the test.checkpoint argument
# here or via CLI, i.e: python test.py +test.checkpoint=/path/to/checkpoint.ckpt
#  checkpoint: /path/to/checkpoint.ckpt 

