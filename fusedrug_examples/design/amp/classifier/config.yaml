name: cls_amp
root: "."
infill: True
target_key: "amp.label" # "amp.label" "toxicity.label"

# data
data:
  batch_size: 256
  target_key: ${target_key}
  infill: ${infill}
  num_batches: 1000

  peptides_datasets: # parameters for PeptidesDatasets constructor
    dbaasp_raw_data_path: ${oc.env:DBAASP_DATA_PATH}
    uniprot_raw_data_path_reviewed: ${oc.env:UNIPROT_PEPTIDE_REVIEWED_DATA_PATH}
    uniprot_raw_data_path_not_reviewed: ${oc.env:UNIPROT_PEPTIDE_NOT_REVIEWED_DATA_PATH}
    toxin_pred_data_path: ${oc.env:TOXIN_PRED_DATA_PATH}
    satpdb_data_path: ${oc.env:SATPDB_DATA_PATH}
    axpep_data_path: ${oc.env:AXPEP_DATA_PATH}
    split_filename: "${root}/split.pickle"
    num_folds: 5
    train_folds: [0, 1, 2]
    validation_folds: [3]
    test_folds: [4]
    seed: 2580
    reset_split: False

  data_loader: # Dataloader constructor parameters
    num_workers: 24

# train
train: # arguments for train() in classifiers_main_train.py
  model_dir: ${root}/${name}
  infill: ${infill}
  target_key: ${target_key}

  # uncomment to track with clearml
  # track_clearml: # arguments for fuse.dl.lightning.pl_funcs.start_clearml_logger
  #   project_name: "pdes"
  #   task_name: ${name}
  #   tags: "fusedrug-examples"
  #   reuse_last_task_id: True
  #   continue_last_task: False

  opt:
    _partial_: true
    # unmask to use SGD instead
    _target_: torch.optim.SGD
    momentum: 0.99
    nesterov: True
    lr: 1e-3
    # weight_decay: 0.1

    # Adam
    # _target_: torch.optim.Adam
    # lr: 1e-3
  # lr_scheduler:
  #   _partial_: true

  #   # _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    
  #   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  #   T_max: ${train.trainer_kwargs.max_epochs}
  #   eta_min: 1e-7 
  #   last_epoch: -1

  trainer_kwargs:
    default_root_dir: ${train.model_dir}
    max_epochs: 1000
    accelerator: "gpu"
    devices: 1
    # strategy: "auto"
    num_sanity_val_steps: 0


# model
model: # arguments for create_model() in classifiers_main_train.py
  max_seq_len: 50
  z_dim: 80
  encoder_type: "transformer" # "transformer", "gru"
  embed:
    emb_dim: 80
  gru_encoder:
    biGRU: True
    layers: 1
    p_dropout: 0.0
    z_dim: ${model.z_dim}
    emb_dim: ${model.embed.emb_dim}
    h_dim: 80
    single_output: True
  transformer_encoder:
    num_tokens: ${model.max_seq_len}
    token_dim: ${model.embed.emb_dim}
    depth: 2
    heads: 10
    mlp_dim: 50
    dropout: 0.1
    emb_dropout: 0.1
    num_cls_tokens: 1
  word_dropout:
    p_word_dropout: 0.1
  random_override:
    p_train: 0.1
  random_adjacent_swap:
    p_train: 0.2
  random_shift:
    max_fraction_train: 1.0
  random_mix:
    p_train: 0.0
  classifier_head:
    num_outputs: 2
    layers_description: [256]
  classifier_recon_head:
    layers_description: [256]
    
hydra:
  run:
    dir: ${root}/${name} 
  job:
    chdir: False 


  
  
  

  