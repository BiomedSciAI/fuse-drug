name: peptides_design
root: "."

# data
data: # arguments for data() function in main_design_train.py
  batch_size: 512

  peptides_datasets: # arguments for PeptidesDatasets constructor
    dbaasp_raw_data_path: ${oc.env:DBAASP_DATA_PATH}
    uniprot_raw_data_path_reviewed: ${oc.env:UNIPROT_PEPTIDE_REVIEWED_DATA_PATH}
    uniprot_raw_data_path_not_reviewed: ${oc.env:UNIPROT_PEPTIDE_NOT_REVIEWED_DATA_PATH}
    toxin_pred_data_path: ${oc.env:TOXIN_PRED_DATA_PATH}
    satpdb_data_path: ${oc.env:SATPDB_DATA_PATH}
    axpep_data_path: ${oc.env:AXPEP_DATA_PATH}
    split_filename: "${root}/split.pickle"
    num_folds: 5
    train_folds: [0, 1, 2]
    validation_folds: [3]
    test_folds: [4]
    seed: 2580
    reset_split: False

  data_loader: # Dataloader constructor parameters
    num_workers: 24


# train
train: # arguments for train() function in main_design_train.py
  model_dir: ${root}/${name}

  # uncomment to track with clearml
  # track_clearml: # arguments for fuse.dl.lightning.pl_funcs.start_clearml_logger
  #   project_name: "pdes"
  #   task_name: ${name}
  #   tags: "fusedrug-examples"
  #   reuse_last_task_id: True
  #   continue_last_task: False
    
  num_iter: 1000

  losses:
    amp_ce:
      weight: 0.5
    toxicity_ce:
      weight: 0.5
    kl_shared_mu:
      weight: 1e-3

  opt:
    _partial_: true
    # # SGD
    # _target_: torch.optim.SGD
    # momentum: 0.9
    # nesterov: True
    # lr: 1e-3

    #ADAM
    _target_: torch.optim.Adam
    lr: 1e-3

  trainer_kwargs:
    default_root_dir: ${train.model_dir}
    max_epochs: 400
    accelerator: "gpu"
    devices: 1
    # strategy: "auto"
    gradient_clip_val: 5.0


# model
model: # arguments for model() function in main_design_train.py
  max_seq_len: 50
  z_dim: 80
  encoder_type: "transformer" # "transformer", "gru"
  decoder_type: "transformer" # "transformer", "gru"
  cls_detached: False
  embed:
    emb_dim: 80
  gru_encoder:
    biGRU: True
    layers: 1
    p_dropout: 0.0
    z_dim: 80
    emb_dim: ${model.embed.emb_dim}
    h_dim: 80
  transformer_encoder:
    num_tokens: ${model.max_seq_len}
    token_dim: ${model.embed.emb_dim}
    depth: 2
    heads: 10
    mlp_dim: 50
    dropout: 0.0
    emb_dropout: 0.0
    num_cls_tokens: 2
  transformer_decoder:
    num_tokens: ${model.max_seq_len}
    token_dim: ${model.z_dim}
    depth: 2
    heads: 10
    mlp_dim: 50
    dropout: 0.0
    emb_dropout: 0.0
    out_dropout: 0.3
  gru_decoder:
    num_tokens: ${model.max_seq_len}
    emb_dim: ${model.z_dim} 
    h_dim: ${model.z_dim}
  word_dropout:
    p_word_dropout: 0.1
  random_override:
    p_train: 0.1
  random_adjacent_swap:
    p_train: 0.2
  random_shift:
    max_fraction_train: 1.0
  random_mix:
    p_train: 0.0
  classification_amp:
    num_outputs: 2
    layers_description: []
  classification_toxicity:
    num_outputs: 2
    layers_description: []
    

hydra:
  run:
    dir: ${root}/${name} 
  job:
    chdir: False 

  
  

  